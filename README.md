
# ğŸ“¦ Ecommerce Analytics avec Spark & Scala

Projet acadÃ©mique de Master 1 Intelligence Artificielle (DIT)  
**Ã‰tudiant : Moussa MallÃ©**  
**Email :** mallemoussa091@gmail.com

---

## ğŸ“š Sujet

DÃ©velopper un systÃ¨me dâ€™analyse de donnÃ©es e-commerce complet en utilisant Apache Spark avec Scala.

Objectifs :

- Ingestion de donnÃ©es multi-format (CSV, JSON, Parquet)
- Nettoyage et validation
- Enrichissement avancÃ© via UDF et fonctions de fenÃªtrage
- Analyse business (KPI, cohortes)
- Optimisations Spark (cache, persist, broadcast)
- Export des rÃ©sultats en CSV et Parquet

---

## ğŸ“ Structure du projet


```

EcommerceAnalytics/  
â”œâ”€â”€ data/ # Fichiers dâ€™entrÃ©e  
â”œâ”€â”€ output/ # RÃ©sultats finaux  
â”œâ”€â”€ src/  
â”‚ â””â”€â”€ main/  
â”‚ â””â”€â”€ scala/com/ecommerce/analytics/  
â”‚ â”œâ”€â”€ MainApp.scala  
â”‚ â”œâ”€â”€ DataIngestion.scala  
â”‚ â”œâ”€â”€ DataTransformation.scala  
â”‚ â”œâ”€â”€ Analytics.scala  
â”‚ â””â”€â”€ models.scala  
â”œâ”€â”€ application.conf # Configuration externe (Typesafe Config)  
â”œâ”€â”€ build.sbt # Fichier de build SBT  
â””â”€â”€ README.md # Ce document

```

---

## ğŸ› ï¸ PrÃ©requis

- Java 11
- Scala 2.11.12
- Spark 2.2.1
- SBT
- Spark (via dÃ©pendances `spark-core`, `spark-sql`)

---

## âš™ï¸ Configuration

Le fichier `application.conf` centralise tous les chemins d'entrÃ©e/sortie :

```hocon
app {
  name = "EcommerceAnalytics"
  env = "dev"
  spark {
    master = "local[*]"
    appName = "EcommerceAnalyticsApp"
  }
  data {
    input {
      transactions = "data/transactions.csv"
      merchants = "data/merchants.csv"
      users = "data/users.json"
      products = "data/products.parquet"
    }
    output {
      path = "output/results"
    }
  }
}

```

----------

## ğŸš€ ExÃ©cution

1.  Cloner le projet :


```bash
git clone https://github.com/codeangel223/DataEngineering-EcommerceAnalytics.git EcommerceAnalytics
cd EcommerceAnalytics

```

2.  Placer les donnÃ©es dans le dossier `data/`.

3.  Lancer :
```bash
sbt run
```

----------

## âœ¨ FonctionnalitÃ©s ImplÃ©mentÃ©es en DÃ©tail

Le projet couvre lâ€™ensemble dâ€™un pipeline analytique Spark, structurÃ© en 7 parties, avec implÃ©mentation complÃ¨te et optimisations professionnelles.


### ğŸ”§ Partie 1 â€“ Configuration & Structure du Projet

-   Mise en place dâ€™une **architecture modulaire** : `models`, `analytics`, `transformation`, `ingestion`, `app`

-   Configuration de `build.sbt` :

    -   Scala 2.11.12 compatible avec Spark 2.2.1

    -   Ajout de `spark-core`, `spark-sql`, et `typesafe-config`

-   Chargement des paramÃ¨tres via un fichier `application.conf` propre


----------

### ğŸ“¥ Partie 2 â€“ Ingestion & Validation des DonnÃ©es

-   Lecture multi-format :

    -   `CSV` : transactions & merchants

    -   `JSON` : users

    -   `Parquet` : products

-   CrÃ©ation de `case class` typÃ©es pour chaque entitÃ©

-   Validation mÃ©tier :

    -   Transactions : `amount > 0`, timestamp de 14 caractÃ¨res

    -   Users : `age âˆˆ [16, 100]`, `income > 0`

    -   Products : `price > 0`, `rating âˆˆ [1, 5]`

    -   Merchants : `commission_rate âˆˆ [0, 1]`

-   Gestion dâ€™erreurs :

    -   Try/catch des erreurs de lecture

    -   Affichage du nombre de lignes lues & valides


----------

### ğŸ§  Partie 3 â€“ Transformations AvancÃ©es

-   **UDF extractTimeFeatures** : Ã  partir dâ€™un timestamp (`yyyyMMddHHmmss`), calcul des dimensions :

    -   Heure, jour, mois, `is_weekend`, `is_working_hour`, `day_period`

-   Enrichissement des transactions :

    -   Jointure avec users, merchants, products

    -   Ajout du `rang` de transaction par utilisateur (Window)

    -   Comptage total de transactions

    -   Classification par **tranche dâ€™Ã¢ge** : Jeune, Adulte, Ã‚ge Moyen, Senior

-   DÃ©tection de comportements par **fenÃªtre glissante 7 jours** :

    -   `active_days_7d` : nb de jours oÃ¹ un utilisateur a transigÃ©

    -   `is_active_user_7d` : flag si â‰¥ 5 jours actifs sur 7


----------

### ğŸ“Š Partie 4 â€“ Analytique Business

#### âœ… Rapport KPI par Marchand

-   Chiffre dâ€™affaires total, nombre de clients uniques

-   Montant moyen par transaction

-   Commission totale perÃ§ue

-   Classement par **CA dans sa catÃ©gorie et rÃ©gion** (avec `Window`)

-   RÃ©partition des ventes par tranche dâ€™Ã¢ge


#### ğŸ“ˆ Analyse de Cohortes

-   Identification du mois de **premiÃ¨re transaction** (`cohort_month`)

-   Calcul du taux de **rÃ©tention mensuelle**

-   Construction dâ€™un tableau croisÃ© cohort_month Ã— transaction_month


----------

### ğŸš€ Partie 5 â€“ Optimisations Spark

-   **Gestion mÃ©moire** :

    -   `cache()` sur les DataFrame rÃ©utilisÃ©s

    -   `persist(StorageLevel.MEMORY_AND_DISK_SER)` pour les grands volumes

    -   `unpersist()` aprÃ¨s utilisation

-   **Optimisation des jointures** :

    -   `broadcast()` sur `merchants` pour limiter le shuffle


----------

### ğŸ§© Partie 6 â€“ Application Principale

-   `MainApp.scala` orchestre :

    -   Ingestion, transformation, analyse

    -   Affichage console des rÃ©sultats

    -   Sauvegarde en **CSV** & **Parquet**

-   Configuration centralisÃ©e (`application.conf`)

-   `try/catch/finally` pour robustesse


----------

### âš™ï¸ Partie 7 â€“ Configuration ExternalisÃ©e

-   Chemins des datasets & sortie dÃ©finis dans `application.conf`

-   Variables comme le `app.name` et `spark.master` modifiables facilement
----------

Souhaites-tu que je mette Ã  jour ton `README.md` avec cette nouvelle section enrichie ?

----------

## ğŸ’¾ RÃ©sultats

Les rÃ©sultats sont enregistrÃ©s en double format :

```
output/results/
â”œâ”€â”€ merchant_report/
â”‚   â”œâ”€â”€ csv/
â”‚   â””â”€â”€ parquet/
â””â”€â”€ cohort_report/
    â”œâ”€â”€ csv/
    â””â”€â”€ parquet/

```

----------

## ğŸ“Š Exemple de sorties

### KPI par marchand :

-   Chiffre d'affaires total

-   Nombre de clients uniques

-   Montant moyen par transaction

-   Commission totale perÃ§ue

-   RÃ©partition par tranche dâ€™Ã¢ge

-   Rang dans sa catÃ©gorie et sa rÃ©gion


### Analyse de cohortes :

-   Premier mois dâ€™achat de chaque utilisateur

-   Retention mensuelle post-cohorte


----------

## ğŸ‘¨â€ğŸ“ Ã‰tudiant

-   **Nom** : Moussa MallÃ©

-   **Email** : [mallemoussa091@gmail.com](mailto:mallemoussa091@gmail.com)

-   **Formation** : Master 1 Intelligence Artificielle â€“ DIT


----------

## ğŸ“„ Licence

Projet acadÃ©mique â€“ usage pÃ©dagogique uniquement.

